{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6gL0U1UsscBc0ThmD+gzA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/w1ndwatcher/Transformers/blob/main/01_Creating_a_custom_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of converting text into smaller units called tokens. These tokens can be words, characters, or sub-words."
      ],
      "metadata": {
        "id": "KZuVnwqC2VQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_tcS799o2H1",
        "outputId": "4c336b03-33ce-4c37-8f0b-1344712a68a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, pre_tokenizers, trainers, models"
      ],
      "metadata": {
        "id": "p-AiyCOzpvum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the dataset from the file"
      ],
      "metadata": {
        "id": "HxLGA7UEp7rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/tokenizer_train.txt\", \"r\") as file:\n",
        "  dataset = [line.strip() for line in file.readlines()]\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UARaeA5p_Eg",
        "outputId": "3919ca0d-cce5-4cd8-a22a-9ef3db1630b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Patient complains of persistent headache and dizziness. Prescribed 500mg Paracetamol twice daily.',\n",
              " 'Administer Amoxicillin 250mg every 8 hours for 7 days.',\n",
              " 'Blood pressure reading: 150/95 mmHg. Start Amlodipine 5mg once daily.',\n",
              " 'Diagnosed with Type 2 Diabetes Mellitus. Metformin 850mg after meals.',\n",
              " 'Apply topical Clotrimazole cream twice a day on affected area.',\n",
              " 'Symptoms include nausea, vomiting, and abdominal cramps. Possible food poisoning.',\n",
              " 'Prescribe Ibuprofen 400mg for pain relief every 6 hours as needed.',\n",
              " 'Monitor blood glucose levels before breakfast and dinner.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Byte-Pair (sub-word) tokenizer:-"
      ],
      "metadata": {
        "id": "P33guEttqiuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ],
      "metadata": {
        "id": "vvhFN85pqseH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the pre-tokenizer to split the input into words"
      ],
      "metadata": {
        "id": "ZQ5ti5p6q0nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
      ],
      "metadata": {
        "id": "nQZlUDgcq9H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the BPE tokenizer on the dataset"
      ],
      "metadata": {
        "id": "oT4qLmIqrHvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = trainers.BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer.train_from_iterator(dataset, trainer=trainer)\n",
        "tokenizer.save(\"/content/tokenizer.json\")"
      ],
      "metadata": {
        "id": "_TKyrn9krMw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "bT2h659wu2ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "loTZvlb2vaRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast"
      ],
      "metadata": {
        "id": "FgOQYuFsu6Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/content/tokenizer.json\")"
      ],
      "metadata": {
        "id": "KomqXCyKvF4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Symptoms include\"\n",
        "encoded = tokenizer.encode(text)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqiGvv84vh6o",
        "outputId": "5ce54851-b6a7-49ae-b424-894b5e7f66d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding(num_tokens=2, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoded tokens:-"
      ],
      "metadata": {
        "id": "T47DGRLewTow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz7VMIRQv8gF",
        "outputId": "c311c7a6-dcfe-456f-c43f-a61ccd7366ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Symptoms', 'include']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The patient\"\n",
        "encoded = tokenizer.encode(text)\n",
        "print(encoded.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANx8V0GdwEt2",
        "outputId": "18e4d25e-8d63-4833-ef0f-831dee9ad85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T', 'h', 'e', 'p', 'a', 'tient']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization:-"
      ],
      "metadata": {
        "id": "j6zmuUXDwW2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.tools import EncodingVisualizer"
      ],
      "metadata": {
        "id": "nR1XmfBDwZyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vis = EncodingVisualizer(fast_tokenizer._tokenizer)\n",
        "vis(text=\"The patient\") # 6 tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "c3AkOvEVwgXt",
        "outputId": "a7dfa265-bdfe-4a88-ff43-5a000b438a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <html>\n",
              "        <head>\n",
              "            <style>\n",
              "                .tokenized-text {\n",
              "    width:100%;\n",
              "    padding:2rem;\n",
              "    max-height: 400px;\n",
              "    overflow-y: auto;\n",
              "    box-sizing:border-box;\n",
              "    line-height:4rem; /* Lots of space between lines */\n",
              "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
              "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
              "    background-color: rgba(0,0,0,0.01);\n",
              "    letter-spacing:2px; /* Give some extra separation between chars */\n",
              "}\n",
              ".non-token{\n",
              "    /* White space and other things the tokenizer ignores*/\n",
              "    white-space: pre;\n",
              "    letter-spacing:4px;\n",
              "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
              "    border-bottom:1px solid #A0A0A0;\n",
              "    line-height: 1rem;\n",
              "    height: calc(100% - 2px);\n",
              "}\n",
              "\n",
              ".token {\n",
              "    white-space: pre;\n",
              "    position:relative;\n",
              "    color:black;\n",
              "    letter-spacing:2px;\n",
              "}\n",
              "\n",
              ".annotation{\n",
              "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
              "    border-radius:4px;\n",
              "    position:relative;\n",
              "    width:fit-content;\n",
              "}\n",
              ".annotation:before {\n",
              "    /*The before holds the text and the after holds the background*/\n",
              "    z-index:1000; /* Make sure this is above the background */\n",
              "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
              "    color:white;\n",
              "    position:absolute;\n",
              "    font-size:1rem;\n",
              "    text-align:center;\n",
              "    font-weight:bold;\n",
              "\n",
              "    top:1.75rem;\n",
              "    line-height:0;\n",
              "    left:0;\n",
              "    width:100%;\n",
              "    padding:0.5rem 0;\n",
              "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
              "    overflow: hidden;\n",
              "    white-space: nowrap;\n",
              "    text-overflow:ellipsis;\n",
              "}\n",
              "\n",
              ".annotation:after {\n",
              "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
              "    position:absolute;\n",
              "    font-size:0.75rem;\n",
              "    text-align:center;\n",
              "    font-weight:bold;\n",
              "    text-overflow:ellipsis;\n",
              "    top:1.75rem;\n",
              "    line-height:0;\n",
              "    overflow: hidden;\n",
              "    white-space: nowrap;\n",
              "\n",
              "    left:0;\n",
              "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
              "\n",
              "    padding:0.5rem 0;\n",
              "    /* Nast hack below:\n",
              "    We set the annotations color in code because we don't know the colors at css time.\n",
              "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
              "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
              "    can get the color with currentColor.\n",
              "    Annotations wrap tokens and tokens set the color back to black\n",
              "     */\n",
              "    background-color: currentColor;\n",
              "}\n",
              ".annotation:hover::after, .annotation:hover::before{\n",
              "    /* When the user hovers over an annotation expand the label to display in full\n",
              "     */\n",
              "    min-width: fit-content;\n",
              "}\n",
              "\n",
              ".annotation:hover{\n",
              "    /* Emphasize the annotation start end with a border on hover*/\n",
              "    border-color: currentColor;\n",
              "    border: 2px solid;\n",
              "}\n",
              ".special-token:not(:empty){\n",
              "    /*\n",
              "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
              "     */\n",
              "    position:relative;\n",
              "}\n",
              ".special-token:empty::before{\n",
              "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
              "    content:attr(data-stok);\n",
              "    background:#202020;\n",
              "    font-size:0.75rem;\n",
              "    color:white;\n",
              "    margin: 0 0.25rem;\n",
              "    padding: 0.25rem;\n",
              "    border-radius:4px\n",
              "}\n",
              "\n",
              ".special-token:not(:empty):before {\n",
              "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
              "    content:attr(data-stok);\n",
              "    position:absolute;\n",
              "    bottom:1.75rem;\n",
              "    min-width:100%;\n",
              "    width:100%;\n",
              "    height:1rem;\n",
              "    line-height:1rem;\n",
              "    font-size:1rem;\n",
              "    text-align:center;\n",
              "    color:white;\n",
              "    font-weight:bold;\n",
              "    background:#202020;\n",
              "    border-radius:10%;\n",
              "}\n",
              "/*\n",
              "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
              "instead we apply even and odd class at generation time and color them that way\n",
              " */\n",
              ".even-token{\n",
              "    background:#DCDCDC\t;\n",
              "    border: 1px solid #DCDCDC;\n",
              "}\n",
              ".odd-token{\n",
              "    background:#A0A0A0;\n",
              "    border: 1px solid #A0A0A0;\n",
              "}\n",
              ".even-token.multi-token,.odd-token.multi-token{\n",
              "    background:  repeating-linear-gradient(\n",
              "    45deg,\n",
              "    transparent,\n",
              "    transparent 1px,\n",
              "    #ccc 1px,\n",
              "    #ccc 1px\n",
              "    ),\n",
              "    /* on \"bottom\" */\n",
              "    linear-gradient(\n",
              "    to bottom,\n",
              "    #FFB6C1,\n",
              "    #999\n",
              "    );\n",
              "}\n",
              "\n",
              ".multi-token:hover::after {\n",
              "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
              "    color:white;\n",
              "    background-color: black;\n",
              "    position:absolute;\n",
              "    font-size:0.75rem;\n",
              "    text-align:center;\n",
              "    font-weight:bold;\n",
              "    text-overflow:ellipsis;\n",
              "    top:1.75rem;\n",
              "    line-height:0;\n",
              "    overflow: hidden;\n",
              "    white-space: nowrap;\n",
              "    left:0;\n",
              "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
              "    padding:0.5rem 0;\n",
              "}\n",
              "\n",
              "            </style>\n",
              "        </head>\n",
              "        <body>\n",
              "            <div class=\"tokenized-text\" dir=auto>\n",
              "            <span class=\"token even-token\"  >T</span><span class=\"token odd-token\"  >h</span><span class=\"token even-token\"  >e</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >p</span><span class=\"token even-token\"  >a</span><span class=\"token odd-token\"  >tient</span>\n",
              "            </div>\n",
              "        </body>\n",
              "    </html>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "2eodH3PfyQB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a pre-trained tokenizer:-"
      ],
      "metadata": {
        "id": "MKN-dirwySPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "rHPupiw8yRN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "_ZLpaFHqzG7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.tokenize(\"The patient\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS5P_8lxzCzn",
        "outputId": "10c6e82b-939e-4f1d-acee-6d19542712b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'patient']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.tokenize(\"The patients daily doses of meds\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-1-s-Pxycoa",
        "outputId": "d5600a6a-0abd-49b9-ea9e-d36ae05b8119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'patients', 'daily', 'doses', 'of', 'med', '##s']\n"
          ]
        }
      ]
    }
  ]
}